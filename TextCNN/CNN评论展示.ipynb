{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_on4eyh2",
    "id": "9E73B21214794A1B8F92AF2169A6F409",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 推荐评论展示任务\n",
    "\n",
    "**任务描述**<br>\n",
    "本次推荐评论展示任务的目标是从真实的用户评论中，挖掘合适作为推荐理由的短句。点评软件展示的推荐理由具有长度限制，而真实用户评论语言通顺、信息完整。综合来说，两者都具有用户情感的正负向，但是展示推荐理由的内容相关性高于评论，需要较强的文本吸引力。\n",
    "\n",
    "**数据集**<br>\n",
    "本次推荐评论展示任务所采用的数据集是点评软件中，用户中文评论的集合。\n",
    "\n",
    "**数据样例**<br>\n",
    "本次任务要求将这些评论分为两类，即“展示”和“不展示”，分别以数字1和0作为标注，如下图所示：\n",
    "\n",
    "**文档说明**<br>\n",
    "数据集文件分为训练集和测试集部分，对应文件如下：\n",
    "\n",
    "- 带标签的训练数据：`train_shuffle.txt` \n",
    "- 不带标签的测试数据：`test_handout.txt`\n",
    "\n",
    "`test_handout.txt`文件的行索引从0开始，对应于ID一列，评论内容为“展示”的预测概率应于Prediction一列。\n",
    "\n",
    "需要注意的是，由于数据在标注时存在主观偏好，标记为“不展示”（0）的评论不一定是真正的负面评论，反之亦然。但是这种情况的存在，不会对任务造成很大的歧义，通过基准算法我们可以在测试集上实现很高的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:50.720659Z",
     "start_time": "2020-03-01T15:12:50.040693Z"
    },
    "graffitiCellId": "id_qpkl0v3",
    "id": "4C34DE8574664EBC876799AFFD97FACC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import torch\n",
    "import torchtext.data as data\n",
    "from torchtext.vocab import Vectors\n",
    "import sys \n",
    "import re\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:50.760410Z",
     "start_time": "2020-03-01T15:12:50.723165Z"
    },
    "graffitiCellId": "id_nxrjw92",
    "id": "2D650272040243AA8AC04AA8BCC96645",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>酸菜鱼不错</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>轻食素食都是友善的饮食方式</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>完爆中午吃的农家乐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>烤鱼很入味</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>有种入口即化的感觉</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL           TEXT\n",
       "0      0          酸菜鱼不错\n",
       "1      0  轻食素食都是友善的饮食方式\n",
       "2      0      完爆中午吃的农家乐\n",
       "3      1          烤鱼很入味\n",
       "4      0      有种入口即化的感觉"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/train.csv', header=0)\n",
    "valid = pd.read_csv('/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/valid.csv', header=0)\n",
    "test = pd.read_csv('/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/test.csv', header=0)\n",
    "train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_q12pdj4",
    "id": "27D596E884AD48778032BE5876EA56C7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 预处理数据\n",
    "## 词语切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:50.857104Z",
     "start_time": "2020-03-01T15:12:50.762591Z"
    },
    "id": "64534CE8B1F84514994D9A210F255885",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_word_vectors(model_name, model_path):\n",
    "    vectors = Vectors(name=model_name, cache=model_path)\n",
    "    print(len(vectors))\n",
    "    return vectors\n",
    "\n",
    "regex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9]')\n",
    "def word_cut(text):\n",
    "    text = regex.sub(' ', text)\n",
    "    return [word for word in jieba.cut(text) if word.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:52.757977Z",
     "start_time": "2020-03-01T15:12:50.862342Z"
    },
    "graffitiCellId": "id_f6u98c3",
    "id": "501A5FC9F17D49B084DE9C15B04233A3",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.694 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in vocab: 2510\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "def read_train(data_root):\n",
    "    label, train = [], []\n",
    "    with open(data_root, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            label.append(int(line.split('\\t')[0]))\n",
    "            train.append(list(jieba.cut(line.split('\\t')[1])))\n",
    "    return label, train\n",
    "train_label, train = read_train('/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/review_train.txt')\n",
    "valid_label, valid = read_train('/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/review_valid.txt')\n",
    "# all_label, all_train = read_train('/home/kesci/train_shuffle.txt')\n",
    "\n",
    "test = []\n",
    "with open('/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/test_handout.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        test.append(list(jieba.cut(line)))\n",
    "\n",
    "def get_vocab(train):\n",
    "    '''\n",
    "    @params: data: 同上\n",
    "    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n",
    "    @counter: Counter({'的': 6935, '很': 3553, '不错': 2728, '恰到好处': 2243, '好': 1851, '味道': 1590, '了': 872, '都': 826, '好吃': 817, \n",
    "                     '是': 806, '一如既往': 770, '和': 711, '非常': 634, '装修': 514, '环境': 514, '还': 486, '吃': 484, '特别': 412, \n",
    "    '''\n",
    "    counter = collections.Counter([word for sen in train for word in sen])\n",
    "#     print(counter)\n",
    "    return Vocab.Vocab(counter, min_freq=3)\n",
    "\n",
    "vocab = get_vocab(train)\n",
    "print('# words in vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:52.770260Z",
     "start_time": "2020-03-01T15:12:52.759953Z"
    },
    "code_folding": [
     19
    ]
   },
   "outputs": [],
   "source": [
    "def MyDataset(root, state='Train', k=0):\n",
    "    if state == 'Train':\n",
    "        path = root + 'train_shuffle.csv'\n",
    "        train_data = pd.read_csv(path, names = ['all'], header = None)\n",
    "        data = pd.concat(\n",
    "            [train_data[:int((k % 5) * len(train_data) / 5)],       # 10折：[:0]; [:1]\n",
    "             train_data[int((k % 5 + 1) * len(train_data) / 5):]])  # 10折：[1:]; [2:]\n",
    "        \n",
    "        data['label'] = data['all'].str[0]\n",
    "        data['text'] = data['all'].str[2:]\n",
    "        data.pop('all')\n",
    "        data = data.reset_index(drop = True)\n",
    "\n",
    "        label = data['label'].tolist()\n",
    "        label = [int(x) for x in label]\n",
    "        text = []\n",
    "        for sen in data['text'].values:\n",
    "            text.append(list(jieba.cut(sen)))\n",
    "\n",
    "    if state == 'Valid':\n",
    "        path = root + 'train_shuffle.csv'\n",
    "        train_data = pd.read_csv(path, names = ['all'], header = None)\n",
    "        data = train_data[int((k % 10) * len(train_data) /10) : int((k % 10 + 1) * len(train_data) /10)]# [:1]; [1:2]\n",
    "        \n",
    "        data['label'] = data['all'].str[0]\n",
    "        data['text'] = data['all'].str[2:]\n",
    "        data.pop('all')\n",
    "        data = data.reset_index(drop = True)\n",
    "\n",
    "        label = data['label'].tolist()\n",
    "        label = [int(x) for x in label]\n",
    "        text = []\n",
    "        for sen in data['text'].values:\n",
    "            text.append(list(jieba.cut(sen)))\n",
    "    \n",
    "    return label, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T10:16:23.358368Z",
     "start_time": "2020-03-01T10:16:22.455506Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "root = '/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/'\n",
    "train_k0_label, train_k0_text = MyDataset(root, state='Train', k=0)\n",
    "valid_k0_label, valid_k0_text = MyDataset(root, state='Valid', k=0)\n",
    "# train_k0_label\n",
    "# type(train_k0_text)\n",
    "# len(train_k0_text)\n",
    "# valid_k0_label\n",
    "valid_k0_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T10:16:23.366024Z",
     "start_time": "2020-03-01T10:16:23.360238Z"
    },
    "deletable": false,
    "editable": false,
    "id": "4B665195047343978720BC0802EC7E65",
    "jupyter": {},
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[:5]\n",
    "# len(all_train)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_9s26xhr",
    "id": "B6CDF2CC978D48F980E835392BAC8CF0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "词典和词语的索引创建好后，就可以将数据集的文本从字符串的形式转换为单词下标序列的形式，以待之后的使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本截取（补0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:53.052148Z",
     "start_time": "2020-03-01T15:12:52.772158Z"
    },
    "graffitiCellId": "id_3ejykvx",
    "id": "B8BDA6B5EE364F538022F97E8A1114FC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    3,  219,  ...,    0,    0,    0],\n",
       "        [ 331,   93,    0,  ...,    0,    0,    0],\n",
       "        [   7,  871,  351,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  28,  112,    6,  ...,    0,    0,    0],\n",
       "        [  30,   14,    4,  ...,    0,    0,    0],\n",
       "        [1020,  108,    2,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上，原始的读入数据\n",
    "        vocab: 训练集上生成的词典\n",
    "    @return:\n",
    "        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n",
    "        labels: 情感标签，形状为 (n,) 的0/1整数张量\n",
    "    '''\n",
    "    max_l = 12  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in data])\n",
    "    return features\n",
    "\n",
    "train_ = preprocess_imdb(train, vocab)\n",
    "train_y_ = torch.tensor(train_label)\n",
    "valid_ = preprocess_imdb(valid, vocab)\n",
    "valid_y_ = torch.tensor(valid_label)\n",
    "pre_ = preprocess_imdb(test, vocab)\n",
    "\n",
    "all_train_ = preprocess_imdb(train, vocab)\n",
    "all_train_y = torch.tensor(train_label)\n",
    "pre_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_yge4ncq",
    "id": "D4189672985F4DA781725897C3395EBB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 创建数据迭代器\n",
    "\n",
    "利用 [`torch.utils.data.TensorDataset`](https://pytorch.org/docs/stable/data.html?highlight=tensor%20dataset#torch.utils.data.TensorDataset)，可以创建 PyTorch 格式的数据集，从而创建数据迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:53.088742Z",
     "start_time": "2020-03-01T15:12:53.053798Z"
    },
    "graffitiCellId": "id_q2o053r",
    "id": "46970B8D972042009CC3393C59BCD4B6",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 12]) y torch.Size([64])\n",
      "#batches: 219\n"
     ]
    }
   ],
   "source": [
    "train_set = Data.TensorDataset(train_, train_y_)\n",
    "test_set = Data.TensorDataset(valid_, valid_y_)\n",
    "pre_set = Data.TensorDataset(pre_)\n",
    "\n",
    "# 上面的代码等价于下面的注释代码\n",
    "# train_features, train_labels = preprocess_imdb(train_data, vocab)\n",
    "# test_features, test_labels = preprocess_imdb(test_data, vocab)\n",
    "# train_set = Data.TensorDataset(train_features, train_labels)\n",
    "# test_set = Data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "# len(train_set) = features.shape[0] or labels.shape[0]\n",
    "# train_set[index] = (features[index], labels[index])\n",
    "# print(train_set)\n",
    "\n",
    "batch_size = 64\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "pre_iter = Data.DataLoader(pre_set, 1)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "print('#batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义预处理函数（交叉验证使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:53.176384Z",
     "start_time": "2020-03-01T15:12:53.094579Z"
    }
   },
   "outputs": [],
   "source": [
    "def package(vocab, state, k):  # state='Train','Valid'; k=0~9\n",
    "    label, spl_train = MyDataset('/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/', state, k)\n",
    "    idx_train = preprocess_imdb(spl_train, vocab)\n",
    "    train_y = torch.tensor(label)\n",
    "    train_set = Data.TensorDataset(idx_train, train_y)\n",
    "    batch_size = 64\n",
    "    train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "#     for X, y in train_iter:\n",
    "#         print('X', X.shape, 'y', y.shape)\n",
    "#         break\n",
    "#     print('#batches:', len(train_iter))\n",
    "    return train_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_rfun1jp",
    "id": "6C1065F141F041D68C843ECD13DB5193",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 训练模型\n",
    "训练时可以调用之前编写的 `train` 及 `evaluate_accuracy` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:53.332711Z",
     "start_time": "2020-03-01T15:12:53.182120Z"
    },
    "graffitiCellId": "id_jv4ye1d",
    "id": "6E7F2873372E42F0AD93F544A6DEEBDC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames):\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    pre, lab = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            pre.append(y_hat)\n",
    "            lab.append(y)\n",
    "            \n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "    return pre, lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_h9301aq",
    "id": "467567D6536D47B887CF000BF0B9E7B1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# TextCNN模型\n",
    "## MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:53.612494Z",
     "start_time": "2020-03-01T15:12:53.336138Z"
    },
    "graffitiCellId": "id_3r1xhqh",
    "id": "CB9416F229DB46038A3FC9AD670096E3",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @params:\n",
    "            x: 输入，形状为 (batch_size, n_channels, seq_len) 的张量\n",
    "        @return: 时序最大池化后的结果，形状为 (batch_size, n_channels, 1) 的张量\n",
    "        '''\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2]) # kenerl_size=seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ksnuxvt",
    "id": "331B28609E2F4FCD81A30F5E665DB7EF",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 加载预训练的词向量\n",
    "\n",
    "由于预训练词向量的词典及词语索引与我们使用的数据集并不相同，所以需要根据目前的词典及索引的顺序来加载预训练词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:12:53.969749Z",
     "start_time": "2020-03-01T15:12:53.616827Z"
    },
    "graffitiCellId": "id_1x32tei",
    "id": "E1E904AB724240589BAF01DA77485092",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_dir = '/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/.vector_cache/'\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir)\n",
    "\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n",
    "        pretrained_vocab: 预训练词向量\n",
    "    @return:\n",
    "        embed: 加载到的词向量\n",
    "    '''\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38138911947E43898EDFDA6CF89D9DC3",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## TextCNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:15:10.523889Z",
     "start_time": "2020-03-01T15:15:10.208693Z"
    },
    "code_folding": [],
    "graffitiCellId": "id_9mqnlf7",
    "id": "43E465F855D841EEA2ABD3F0B16F6429",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 190 oov words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.6517,  0.5360,  0.3403,  ...,  0.8054,  0.1046,  0.1937],\n",
       "        ...,\n",
       "        [ 0.0179, -0.2923, -0.2269,  ...,  0.4407,  0.9959, -0.2199],\n",
       "        [ 0.0693, -0.1167,  0.1685,  ...,  0.2784,  0.0289, -0.0813],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            kernel_sizes: 卷积核大小列表\n",
    "            num_channels: 卷积通道数列表\n",
    "        '''\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size) # 参与训练的嵌入层\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size) # 不参与训练的嵌入层\n",
    "        \n",
    "        self.pool = GlobalMaxPool1d() # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = 2*embed_size, \n",
    "                                        out_channels = c, \n",
    "                                        kernel_size = k))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        self.dropout = nn.Dropout(0.5) # 丢弃层用于防止过拟合\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n",
    "        @return:\n",
    "            outputs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n",
    "        '''\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), \n",
    "            self.constant_embedding(inputs)), dim=2) # (batch_size, seq_len, 2*embed_size)\n",
    "        # 根据一维卷积层要求的输入格式，需要将张量进行转置\n",
    "        embeddings = embeddings.permute(0, 2, 1) # (batch_size, 2*embed_size, seq_len)\n",
    "        \n",
    "        encoding = torch.cat([\n",
    "            self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        # encoding = []\n",
    "        # for conv in self.convs:\n",
    "        #     out = conv(embeddings) # (batch_size, out_channels, seq_len-kernel_size+1)\n",
    "        #     out = self.pool(F.relu(out)) # (batch_size, out_channels, 1)\n",
    "        #     encoding.append(out.squeeze(-1)) # (batch_size, out_channels)\n",
    "        # encoding = torch.cat(encoding) # (batch_size, out_channels_sum)\n",
    "        \n",
    "        # outputs = self.softmax(self.dropout(encoding))\n",
    "        # print()\n",
    "        # 应用丢弃法后使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "\n",
    "embed_size, kernel_sizes, nums_channels = 300, [3, 4], [100, 100]\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)\n",
    "\n",
    "zhihu = Vectors(name='sgns.zhihu.bigram', cache='/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/')\n",
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, zhihu))\n",
    "\n",
    "# # net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, zhihu))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_leupmp8",
    "id": "374844FBE56B43668639A9C07D0E2DD0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 训练并评价模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:16:48.880536Z",
     "start_time": "2020-03-01T15:16:36.227209Z"
    },
    "graffitiCellId": "id_g74v4mq",
    "id": "BC1674BE51704FBA9F46925EE338B813",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.4217, train acc 0.888, test acc 0.874, time 1.6 sec\n",
      "epoch 2, loss 0.2099, train acc 0.889, test acc 0.877, time 1.6 sec\n",
      "epoch 3, loss 0.1398, train acc 0.889, test acc 0.877, time 1.6 sec\n",
      "epoch 4, loss 0.1046, train acc 0.892, test acc 0.873, time 1.6 sec\n",
      "epoch 5, loss 0.0840, train acc 0.887, test acc 0.869, time 1.6 sec\n",
      "epoch 6, loss 0.0698, train acc 0.887, test acc 0.870, time 1.6 sec\n",
      "epoch 7, loss 0.0595, train acc 0.893, test acc 0.874, time 0.8 sec\n",
      "epoch 8, loss 0.0522, train acc 0.891, test acc 0.869, time 0.7 sec\n",
      "epoch 9, loss 0.0464, train acc 0.888, test acc 0.875, time 0.7 sec\n",
      "epoch 10, loss 0.0415, train acc 0.894, test acc 0.870, time 0.7 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 10\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# for k in range(5):\n",
    "#     t_iter = package(vocab, 'Train', k)\n",
    "#     v_iter = package(vocab, 'Valid', k)\n",
    "#     pre, lab = train(t_iter, v_iter, net, loss, optimizer, device, num_epochs)\n",
    "\n",
    "pre, lab = train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T10:18:11.718805Z",
     "start_time": "2020-03-01T10:18:11.705158Z"
    },
    "deletable": false,
    "editable": false,
    "id": "3F61A2FD99A540328D76E445DD5D4B9D",
    "jupyter": {},
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[:5]\n",
    "pre_[:5]\n",
    "# tensor([[   0,    3,  219,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
    "#         [ 331,   93, 3438,   17,   14,  155,    0,    0,    0,    0,    0,    0],\n",
    "#         [   7,  871,  351,    5,    0,    0,    0,    0,    0,    0,    0,    0],\n",
    "#         [8491,   27,    2,    5,    0,    0,    0,    0,    0,    0,    0,    0],\n",
    "#         [ 548,  219,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T12:24:53.619058Z",
     "start_time": "2020-03-01T12:24:53.605944Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv (\"/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/submission.csv\" , encoding = \"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T10:25:12.925123Z",
     "start_time": "2020-03-01T10:25:08.921801Z"
    },
    "id": "39E65D673DE94188A9D5EBC6045874E1",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "print(\"training on \", device)\n",
    "pre = []\n",
    "for X in pre_iter:\n",
    "    # print(X)\n",
    "    X = torch.tensor(X[0])\n",
    "    X = X.to(device)\n",
    "    y_hat = net(X)\n",
    "    pre.append(y_hat.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T10:25:14.422434Z",
     "start_time": "2020-03-01T10:25:14.381728Z"
    },
    "id": "9C488730382D459485475900A792EB61",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = []\n",
    "for j in range(len(pre)):\n",
    "    item = pre[j][0][1].item()\n",
    "    sub.append(item)\n",
    "\n",
    "# len(pre)\n",
    "# pre[5][0][1].item()\n",
    "sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cnblogs.com/everfight/p/create_dataframe_from_different_type.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T10:18:36.103470Z",
     "start_time": "2020-03-01T10:18:36.076961Z"
    },
    "id": "9B0A4D5370724EDB8040BD76230CA6DC",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ID = []\n",
    "for i in range(0, 4189):\n",
    "    ID.append(i)\n",
    "ID[-1]\n",
    "list_ = zip(ID, sub)\n",
    "list_\n",
    "# diction = dict(zip(ID,sub))\n",
    "# diction\n",
    "output = pd.DataFrame.from_records(list_, columns=['ID', 'Prediction'])\n",
    "# output\n",
    "# df = pd.DataFrame.from_records(sub, columns=['Prediction'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T10:18:44.259965Z",
     "start_time": "2020-03-01T10:18:44.213836Z"
    },
    "id": "0AC1CF58A8FA49A68657A9925A0D3D53",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# index=False\n",
    "output.to_csv (\"/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/submission.csv\" , encoding = \"utf-8\", index=False)\n",
    "dfr = pd.read_csv (\"/home/cc/holdshy/XJQ/Pytorch/Dive_into_DL/text_cnn/submission.csv\" , encoding = \"utf-8\")\n",
    "print (dfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1E57410744204F349AC5131BAB171BFE",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "506.667px",
    "left": "1103px",
    "top": "141.63px",
    "width": "218.681px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
